/**
 * ModelProvider – fetches model metadata from supported providers and caches it.
 *
 * Each provider has its own endpoint that returns a list of available models.
 * The result is stored in a JSON file under the project cache directory so that
 * subsequent calls within the cache TTL do not hit the network.
 *
 * The public API:
 *   - `getAll()` – returns a `Map<string, ModelInfo>`
 *     with the union of all provider models (local + remote).
 *
 * @module llm/ModelProvider
 */
import { FileSystem } from "../utils/index.js"

import CerebrasInfo from "./providers/cerebras.info.js"
import HuggingFaceInfo from "./providers/huggingface.info.js"
import OpenrouterInfo from "./providers/openrouter.info.js"
import { ModelInfo } from "./ModelInfo.js"
import { Pricing } from "./Pricing.js"

const transformers = {
	cerebras: CerebrasInfo.makeFlat,
	huggingface: HuggingFaceInfo.makeFlat,
	openrouter: OpenrouterInfo.makeFlat,
}

/** @typedef {"cerebras" | "openrouter" | "huggingface"} AvailableProvider */
/**
 * @typedef {Object} HuggingFaceProviderInfo
 * @property {string} provider
 * @property {string} status
 * @property {number} context_length
 * @property {{ input: number, output: number }} pricing
 * @property {boolean} supports_tools
 * @property {boolean} supports_structured_output
 * @property {boolean} is_model_author
*/

class CacheConfig {
	/** @type {number} Cache duration – 1 hour (in milliseconds) */
	ttl = 60 * 60 * 1e3
	file = "chat/cache/{provider}.jsonl"
	/** @param {Partial<CacheConfig>} [input] */
	constructor(input = {}) {
		const {
			ttl = this.ttl,
			file = this.file,
		} = input
		this.ttl = Number(ttl)
		this.file = String(file)
	}
	/**
	 * @param {string} provider
	 * @return {string}
	 */
	getFile(provider) {
		return this.file.replaceAll("{provider}", provider)
	}
	/**
	 * @param {number} time File change time in milliseconds
	 * @param {number} [now] Now time in milliseconds
	 * @returns {boolean}
	 */
	isAlive(time, now = Date.now()) {
		return (now - time) < this.ttl
	}
}

export class ModelProvider {
	/** @type {AvailableProvider[]} */
	static AvailableProviders = ["cerebras", "huggingface", "openrouter"]
	/** @type {FileSystem} */
	#fs
	/** @type {CacheConfig} */
	#cache

	constructor(input = {}) {
		const {
			fs = new FileSystem(),
			cache = new CacheConfig(),
		} = input
		this.#fs = fs
		this.#cache = cache
	}

	get cachePath() {
		return this.#fs.path.resolve(this.#cache.file)
	}

	get cacheConfig() {
		return this.#cache
	}

	/**
	 * Load the cache file if it exists and is fresh.
	 * @param {string} provider
	 * @returns {Promise<object[] | null>}
	 */
	async loadCache(provider) {
		const file = this.cacheConfig.getFile(provider)
		try {
			if (await this.#fs.access(file)) {
				const rows = await this.#fs.load(file) ?? []
				if (!rows.length) return null
				const stats = await this.#fs.info(file)
				if (this.cacheConfig.isAlive(stats.mtimeMs)) {
					return rows
				}
			}
		} catch (/** @type {any} */ err) {
			// Ignore cache read errors – fall back to fresh fetch.
			console.debug(`Cache load failed: ${err.message}`)
		}
		return null
	}

	/**
	 * Write fresh data to the cache as JSONL (one model per line).
	 * @param {any} data
	 * @param {string} provider
	 */
	async writeCache(data, provider) {
		await this.#fs.save(this.cacheConfig.getFile(provider), data)
	}

	/**
	 * Fetch model list from a provider endpoint.
	 *
	 * The function knows how to call each supported provider.
	 *
	 * @param {AvailableProvider} provider
	 * @returns {Promise<Array<Partial<ModelInfo> & {id: string}>>} Raw model data.
	 */
	async fetchFromProvider(provider) {
		switch (provider) {
			case "cerebras":
				if (!process.env.CEREBRAS_API_KEY) {
					throw new Error("CEREBRAS_API_KEY required for Cerebras models")
				}
				return await this.#jsonFetch(`https://api.cerebras.ai/v1/models`, {
					Authorization: `Bearer ${process.env.CEREBRAS_API_KEY}`
				})
			case "openrouter":
				if (!process.env.OPENROUTER_API_KEY) {
					throw new Error("OPENROUTER_API_KEY required for OpenRouter models")
				}
				return await this.#jsonFetch(`https://openrouter.ai/api/v1/models`, {
					Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`
				})
			case "huggingface":
				const HF_TOKEN = process.env.HF_TOKEN || process.env.HUGGINGFACE_API_KEY
				if (!HF_TOKEN) {
					throw new Error("HF_TOKEN required for Hugging Face models")
				}
				// Note: Hugging Face router API may not provide full model list; fallback to static.
				// Endpoint for inference models: https://huggingface.co/docs/api-inference/models
				try {
					return await this.#jsonFetch(
						"https://router.huggingface.co/v1/models",
						{ Authorization: `Bearer ${HF_TOKEN}` }
					)
				} catch (/** @type {any} */ err) {
					console.debug(`HF fetch failed, using static: ${err.message}`)
					return [] // Fallback to static info only.
				}
			default:
				throw new Error(`Unsupported provider "${provider}"`)
		}
	}

	/**
	 *
	 * @param {string | URL | globalThis.Request} url
	 * @param {RequestInit} options
	 * @returns {Promise<Response>}
	 */
	async fetch(url, options) {
		return await fetch(url, options)
	}

	/**
	 * Generic JSON fetch – uses native fetch (Node ≥ 18) and validates response.
	 * Adds provider‑specific headers (e.g. Authorization for Cerebras).
	 *
	 * @param {string} url
	 * @param {object} [headers={}]
	 * @returns {Promise<Array>}
	 */
	async #jsonFetch(url, headers = {}) {
		const resp = await this.fetch(url, { headers: { ...headers, "Accept": "application/json" } })
		if (!resp.ok) {
			throw new Error(`Failed to fetch ${url}: ${resp.status} ${resp.statusText}`)
		}
		const json = /** @type {any} */ (await resp.json())
		// Providers may return an array directly or wrap it in a property.
		if (Array.isArray(json)) return json
		if (Array.isArray(json.data)) return json.data
		if (Array.isArray(json.models)) return json.models
		// Fallback to empty array – caller will handle absence gracefully.
		return []
	}

	/**
	 * Flatten multi-provider entries into separate ModelInfo instances.
	 * @param {Array<ModelInfo & { providers?: HuggingFaceProviderInfo[] }>} arr
	 * @param {AvailableProvider} provider
	 * @param {Array<[string, Partial<ModelInfo>]>} [predefined]
	 * @returns {ModelInfo[]}
	 */
	_makeFlat(arr, provider, predefined = []) {
		const map = new Map(predefined)
		const result = []
		const push = item => {
			if (item) {
				if ("openrouter" === provider) {
					this.#multiply(item, 1e6)
				}
				result.push(item)
			}
		}
		for (const model of arr) {
			const pre = map.get(model.id) ?? {}
			if (model.providers && Array.isArray(model.providers)) {
				for (const opts of model.providers) {
					const pro = provider + "/" + (opts.provider ?? "")
					if (pro.endsWith("/")) {
						console.warn("Incorrect model's provider: " + pro)
						continue
					}
					const { providers, ...rest } = model
					// @todo transform platform-specific info into ModelInfo before pushing it
					// supports only HuggingFace providers, and OpenRouter flat version
					// @ts-ignore
					push(new ModelInfo({ ...pre, ...rest, ...opts, provider: pro }))
				}
			} else {
				push(new ModelInfo({ ...pre, ...model, provider: provider ?? model.provider }))
			}
		}
		return result
	}

	/**
	 *
	 * @param {ModelInfo} model
	 * @param {number} rate
	 */
	#multiply(model, rate = 1) {
		const fields = [
			"completion",
			"input_cache_read",
			"input_cache_write",
			"internal_reasoning",
			"prompt",
		]
		model.pricing ??= new Pricing({})
		for (const field of fields) {
			if (model.pricing[field] > 0) {
				model.pricing[field] *= rate
			}
		}
		return model
	}

	/**
	 * Return a map of model-id → array of ModelInfo (one per provider variant).
	 *
	 * Attempts cache first. If stale/missing, fetches from providers, merges with static info,
	 * updates cache, and returns. Errors per-provider are swallowed, falling back to static.
	 *
	 * @param {object} [options={}]
	 * @param {function(string, string[]): void} [options.onBefore] Called before fetch.
	 * @param {function(string, any, ModelInfo[]): void} [options.onData] Called after normalization.
	 * @param {boolean} [options.noCache]
	 * @returns {Promise<Map<string, ModelInfo>>}
	 */
	async getAll(options = {}) {
		/**
		 * @param {ModelInfo[]} all
		 * @returns {Map<string, ModelInfo>}
		 */
		const convertMap = (all) => {
			const map = new Map()
			all
				.filter((m) => typeof m.id === "string" && m.id.length > 0)
				.forEach((m) => {
					const id = [m.id, m.provider].join("@")
					map.set(id, m)
				})
			return map
		}

		const {
			onBefore = () => { },
			onData = () => { },
			noCache = false,
		} = options

		/** @type {ModelInfo[]} */
		const all = []

		for (const name of ModelProvider.AvailableProviders) {
			try {
				onBefore(name, ModelProvider.AvailableProviders)
				let raw = []
				// Fetch if possible, else use static only.
				try {
					// Try cache first.
					const cached = noCache ? null : await this.loadCache(name)
					raw = cached ?? await this.fetchFromProvider(name)
					if (!noCache && !cached) await this.writeCache(raw, name)
				} catch (/** @type {any} */ err) {
					console.warn(`Fetch failed for ${name}, using static: ${err.message}`)
					raw = [] // Rely on predefined.
				}

				const flat = this.flatten(raw, name)
				onData(name, raw, flat)
				all.push(...flat)
			} catch (/** @type {any} */ err) {
				console.warn(`Failed to process ${name}: ${err.message}`)
			}
		}

		return convertMap(all)
	}

	/**
	 * @param {Array} raw
	 * @param {AvailableProvider} name
	 * @returns {ModelInfo[]}
	 */
	flatten(raw, name) {
		const transformer = transformers[name] ?? (models => models)
		return transformer(raw)
	}
}

